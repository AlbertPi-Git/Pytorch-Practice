{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import itertools\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device as GPU if available\n",
    "useCuda=torch.cuda.is_available()\n",
    "device=torch.device('cuda:0' if useCuda else 'cpu')\n",
    "torch.backends.cudnn.benchmark=True     # If input images have the same size, enable benchmark can improve runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All params\n",
    "\n",
    "trainEpochs=30\n",
    "\n",
    "# Trainset loader params\n",
    "trainLoadParams = { 'batch_size': 16,\n",
    "                    'shuffle': True,\n",
    "                    'num_workers':8,\n",
    "                    'pin_memory':True}\n",
    "\n",
    "# Testset loader params\n",
    "testLoadParams = { 'batch_size': 16,\n",
    "                    'shuffle': False,\n",
    "                    'num_workers':8,\n",
    "                     'pin_memory':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation to transform PIL image to tensor and normalize the input tensors\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Read trainset and load with dataloader\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset,**trainLoadParams) # ** dic  is a syntax sugar for fitting function parameters with elements of a dic\n",
    "\n",
    "# Read testset and load with dataloader\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset,**testLoadParams)\n",
    "\n",
    "# Corresponding classes labels of CIFAR10, the order is important and shouldn't be changed\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Show some of the training images\n",
    "\n",
    "# Functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.cpu().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Get some random training images\n",
    "example_data, example_targets = next(iter(trainloader))\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(example_data[:4]))\n",
    "\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[example_targets[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Define original AlexNet and my AlexNet variant with FC layers at last replaced by one global average pooling layer\n",
    "\n",
    "\n",
    "# Original AlexNet\n",
    "class AlexNet(nn.Module):   \n",
    "    def __init__(self, num_class=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),   \n",
    "            nn.MaxPool2d( kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 96, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),                         \n",
    "            nn.Conv2d(96, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),                         \n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d( kernel_size=2, stride=1),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(32*14*14,2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048,1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024,num_class),\n",
    "        )\n",
    "        self.lossFunc=nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.feature(x)\n",
    "        x = x.view(-1,32*14*14)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# AlexNet variant with global average pooling\n",
    "class GlobalPoolAlexNet(nn.Module):   \n",
    "    def __init__(self, num_class=10):\n",
    "        super(GlobalPoolAlexNet, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),   \n",
    "            nn.MaxPool2d( kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 96, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),                         \n",
    "            nn.Conv2d(96, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),                         \n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d( kernel_size=2, stride=1),\n",
    "            # Last conv layer output channel number should be class number\n",
    "            nn.Conv2d(32, num_class, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d( kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            # What GAP does is reducing each channel feature map to a single value, and each channel represent a class\n",
    "            nn.AvgPool2d(kernel_size=6), \n",
    "        )\n",
    "        self.lossFunc=nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.feature(x)\n",
    "        x = self.classifier(x).squeeze_()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = GlobalPoolAlexNet().to(device)\n",
    "# model = AlexNet().to(device)\n",
    "\n",
    "# Instatiate optimizer\n",
    "optimizer = optim.SGD(model.parameters(),lr=1e-3,momentum=0.95,weight_decay=1e-3)\n",
    "\n",
    "# Specify the loss function by getting the loss function from the model\n",
    "# ( I think it's easier to change and not lead to bug if loss function is included in network model )\n",
    "lossFunc=model.lossFunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute loss of current model on train set and test set\n",
    "def computeLoss(datasetName):\n",
    "    accum_loss=0\n",
    "    num_sample=0\n",
    "    \n",
    "    # Specify the reduction as sum so that we can easily compute average loss with accum_loss/num_sample\n",
    "    lossFunc=nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    \n",
    "    if datasetName==\"train\":\n",
    "        loader=trainloader\n",
    "    else:\n",
    "        loader=testloader\n",
    "    \n",
    "    for i,(inputs,labels) in enumerate(loader):\n",
    "        if i<=len(testloader)/2: # Only compute the loss on half of train set or test set to save some time\n",
    "            inputs, labels = inputs.to(device,non_blocking=True), labels.to(device,non_blocking=True)\n",
    "           \n",
    "            # Don't need gradient for loss computation\n",
    "            with torch.no_grad():\n",
    "                outputs = model.forward(inputs)\n",
    "                loss = lossFunc(outputs,labels)\n",
    "                accum_loss+=loss\n",
    "                num_sample+=len(inputs)\n",
    "    \n",
    "    return accum_loss/num_sample   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Training\n",
    "\n",
    "# batch loss and batch iteration index container\n",
    "batch_losses=[]\n",
    "batch_iters=[]\n",
    "\n",
    "# train and test loss and epoch iteration index container\n",
    "train_losses=[computeLoss(\"train\")]\n",
    "test_losses=[computeLoss(\"test\")]\n",
    "test_iters=[0]\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(trainEpochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, ( inputs, labels) in enumerate(trainloader, 0):\n",
    "        \n",
    "        inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize + update\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = lossFunc(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate running loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # record batch loss at the beginning of each 1000 mini-batches\n",
    "        if i%1000==0:\n",
    "            batch_losses.append(loss.item())\n",
    "            batch_iters.append(epoch*len(trainloader)+i)\n",
    "            \n",
    "        # record test loss after each epoch\n",
    "        if i==len(trainloader)-1:\n",
    "            train_losses.append(computeLoss(\"train\"))\n",
    "            test_losses.append(computeLoss(\"test\"))\n",
    "            test_iters.append(epoch+1)\n",
    "\n",
    "        # print statistics\n",
    "        if i==len(trainloader)-1:    # print every 2000 mini-batches\n",
    "            print(\"Epoch {}, average training loss: {:.3f}\".format(epoch+1,running_loss/len(trainloader)))\n",
    "            running_loss = 0.0 # Reset running loss\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curve of mini-batch, train set and test set\n",
    "plt.figure(dpi=200)\n",
    "plt.title('Training loss curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0,2.75])\n",
    "plt.yticks(np.arange(0,3,0.25))\n",
    "plt.grid(linestyle='--', linewidth=0.5)\n",
    "batchLossLine,=plt.plot(np.array(batch_iters)/len(trainloader), batch_losses,label=\"Batch loss\")\n",
    "trainLossLine, = plt.plot(test_iters, train_losses,label=\"Train set loss\")\n",
    "testLossLine, = plt.plot(test_iters, test_losses,label=\"Test set loss\")\n",
    "plt.legend(handles=[batchLossLine,trainLossLine,testLossLine])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Evaluate accuracy on train set and test set after training\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Evaluation on train set\n",
    "with torch.no_grad():\n",
    "    for (images, labels) in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the model on the 60000 train images: {:.2f}%'.format(100 * correct / total))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Counter of each class and confusion matrix\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "cmt = torch.zeros(10,10, dtype=torch.int64)\n",
    "\n",
    "# Evaluation on test set\n",
    "with torch.no_grad():\n",
    "    for ( images, labels) in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            cmt[labels[i], predicted[i]] += 1\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "print('Accuracy of the model on the 10000 test images: {:.2f}%'.format(100 * correct / total))\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of {:6} : {:.2f} %'.format(classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(dpi=200)\n",
    "plot_confusion_matrix(cmt.numpy(), classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bit074dd42b42544476bd7c1737742524c6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}